{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "State Farm Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS8lbd7svFaX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import time\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeDxCtGzvFaZ"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC6Sw-OBvFaa"
      },
      "source": [
        "raw_train=pd.read_csv('exercise_06_train.csv')\n",
        "raw_test=pd.read_csv('exercise_06_test.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GMFjgkBvFaa"
      },
      "source": [
        "raw_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3txBYVxJ4ish",
        "outputId": "ed046b84-0e9c-4e32-c86e-a417948e2c5b"
      },
      "source": [
        "print(raw_train.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 101)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9C6UxXFvFab"
      },
      "source": [
        "### Drop NULL values in column x1\n",
        "raw_train_drop = raw_train.dropna(axis=0,subset=['y'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sja3z8_M4ZEP",
        "outputId": "d4d67b1f-b5b8-41c0-ff08-a83f42ef854e"
      },
      "source": [
        "print(raw_train_drop.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 101)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB2A0hoFvFab"
      },
      "source": [
        "### Combine train and test data\n",
        "list_all=[raw_train_drop,raw_test]\n",
        "raw = pd.concat(list_all,ignore_index=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyrgyBZVvFac",
        "outputId": "164d5158-dbe1-4451-8e29-7882d8c8b012"
      },
      "source": [
        "len_train = len(raw_train_drop)\n",
        "len_test =len(raw_test)\n",
        "print('The size of effective training and test dataset is', len_train, len_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of effective training and test dataset is 40000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFumAN76vFac"
      },
      "source": [
        "del(raw_train,raw_test,raw_train_drop)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d82OQdtEvFac",
        "outputId": "a3bb30d9-3acc-4c45-eac8-f64bbaf8f822"
      },
      "source": [
        "### Get percentage of NULL values for each feature\n",
        "Null_list = raw.isnull().sum().sort_values(ascending=False)/float(raw.shape[0])*100\n",
        "print('the pencentage of NUll value in each features are:', Null_list[:10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the pencentage of NUll value in each features are: y      20.000\n",
            "x13     0.034\n",
            "x55     0.034\n",
            "x42     0.034\n",
            "x18     0.032\n",
            "x62     0.030\n",
            "x99     0.030\n",
            "x24     0.030\n",
            "x96     0.028\n",
            "x63     0.028\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13rh8NF4vFad"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEmaiFsxvFad"
      },
      "source": [
        "#remove features not used for modeling\n",
        "del raw['x2']\n",
        "del raw['x3']\n",
        "del raw['x19']\n",
        "\n",
        "#These feature need Nature Language Processing before using, thus increasing the complexity of current model\n",
        "del raw['x10']\n",
        "del raw['x16']\n",
        "del raw['x18']\n",
        "\n",
        "#Remove redundant feature\n",
        "del raw['x8']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BptuDDKrvFad"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sKFwCGJvFad"
      },
      "source": [
        "# Remove $ from dollar amount features\n",
        "def remove_dollar(x):\n",
        "    try:\n",
        "        x = str(x)\n",
        "        return float(x.strip('$').replace(',',''))\n",
        "    except:\n",
        "        return np.nan"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixm0rDK3vFae"
      },
      "source": [
        "raw['x4'] = raw['x4'].apply(remove_dollar)\n",
        "raw['x5'] = raw['x5'].apply(remove_dollar)\n",
        "raw['x6'] = raw['x6'].apply(remove_dollar)\n",
        "raw['x12'] = raw['x12'].apply(remove_dollar)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2L2CNtbvFae"
      },
      "source": [
        "# Convert the variable format from percentage to float\n",
        "def per_float(x):\n",
        "    try:\n",
        "        x = str(x)\n",
        "        return float(x.strip('%'))/100\n",
        "    except:\n",
        "        return np.nan"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBboVyOxvFae"
      },
      "source": [
        "raw['x30'] = raw['x30'].apply(per_float)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU00z3nkvFae"
      },
      "source": [
        "# Create new features to be used in modeling\n",
        "raw['x33'] = raw['x5']/raw['x4']\n",
        "raw['x34'] = raw['x6']/raw['x5']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrVGHYvVvFae"
      },
      "source": [
        "# Convert time from string format to float (Number of years since 1900-01-01)\n",
        "def toYears(x):\n",
        "    try:\n",
        "        x = datetime.datetime.strptime(x, \"%b-%Y\")\n",
        "        x = x-datetime.datetime(1900,1,1)\n",
        "        return x.days/365.0\n",
        "    except:\n",
        "        try:\n",
        "            x = datetime.datetime.strptime(x, \"%b-%y\")\n",
        "            if (x - datetime.datetime(2017,12,31)).days> 0:\n",
        "                x = x-datetime.datetime(2000,1,2)\n",
        "                return x.days/365.0\n",
        "            x = x-datetime.datetime(1900,1,1)\n",
        "            return x.days/365.0        \n",
        "        except:\n",
        "            return np.nan"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1zm2k8lvFaf"
      },
      "source": [
        "raw['x15'] = raw['x15'].apply(toYears)\n",
        "raw['x23'] = raw['x23'].apply(toYears)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTcgvuP4vFaf"
      },
      "source": [
        "# Time difference between issue date and the date opened\n",
        "raw['x35'] = raw['x15']  - raw['x23']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8t1lc0GvFaf"
      },
      "source": [
        "### Set target variable and remove it from input variable list\n",
        "raw_y = raw['y']\n",
        "del raw['y']"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi_rJzTYvFaf"
      },
      "source": [
        "### Split input variables into numerical features and categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaChdLAbvFaf"
      },
      "source": [
        "cat_cols = raw.dtypes[raw.dtypes == 'object'].index\n",
        "num_cols = raw.dtypes[raw.dtypes == 'float64'].index"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLK7tm65vFaf"
      },
      "source": [
        "### Make statistics analysis on target variables, numerical features and categorical features\n",
        "### This cell is very slow to run analysis. Don't run unless you're interested in viewing individual features'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVPAY7dK_ARS"
      },
      "source": [
        "# generate histogram for all features\n",
        "fig= plt.figure(figsize=(5,5))\n",
        "ax1 = fig.add_subplot(1,1,1)\n",
        "raw_y.plot(kind= 'hist',axes =ax1)\n",
        "plt.title('Histgram for interest rate')\n",
        "plt.show()\n",
        "\n",
        "for i in cat_cols:\n",
        "    fig= plt.figure(figsize=(15,5))\n",
        "    ax1 = fig.add_subplot(1,2,1)\n",
        "    raw[i].value_counts().plot(kind= 'bar',axes =ax1)\n",
        "    plt.title('Histgram for feature: %s' %(i))\n",
        "    ax2 = fig.add_subplot(1,2,2)\n",
        "    raw[i].value_counts(normalize = 'True').plot(kind= 'bar', axes =ax2)\n",
        "    plt.title('Histgram for feature: %s (in percentate)' %(i))\n",
        "    plt.show()\n",
        "\n",
        "for i in num_cols:\n",
        "    a = raw[i]\n",
        "    b = a[abs(a - a.mean()) <=3*a.std()]\n",
        "    fig= plt.figure(figsize=(8,5))\n",
        "    b.plot(kind= 'hist',bins = 10)\n",
        "    plt.title('Histgram of %s' %(i))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTLaRZVbvFag"
      },
      "source": [
        "## Make a copy of raw input, will be used later as input variables in the linear regression model \n",
        "raw_bp_linear = copy.deepcopy(raw)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt6Foox8vFag"
      },
      "source": [
        "### Data preparation for tree model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lgoSizAvFag"
      },
      "source": [
        "### Replace the Null value with very large number (10**20), let tree model to interpret by itself\n",
        "for i in num_cols:\n",
        "    raw[i].fillna(10**20,inplace=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEG95sx0vFag"
      },
      "source": [
        "### Label encoding for categorical feature\n",
        "from sklearn import preprocessing\n",
        "LBL = preprocessing.LabelEncoder()\n",
        "dict_list = []\n",
        "for i in cat_cols:\n",
        "    raw[i] = LBL.fit_transform(raw[i].fillna('0'))\n",
        "    j = dict(zip(np.arange(len(LBL.classes_)),LBL.classes_))\n",
        "    k = {i:j}\n",
        "    dict_list.append(k)\n",
        "    \n",
        "# uncomment the following print statement if you want to see the dictionary\n",
        "#print(dict_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtpuV8pDvFah"
      },
      "source": [
        "### Split into training and test dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "x = raw[:len_train]\n",
        "y = raw_y[:len_train]\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.33, random_state=42)\n",
        "holdout_x = raw[len_train:]\n",
        "holdout_y = raw_y[len_train:]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni_JNpRTvFah"
      },
      "source": [
        "### Build random forest model\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYJsHJoEvFah"
      },
      "source": [
        "rfr = RandomForestRegressor(n_jobs=-1) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGdBEcaOvFah"
      },
      "source": [
        "#### hyper-parameter search: n_estimators\n",
        "start to build random forest model\"\n",
        "To reduce the code running time, the process of hypermeter grid search for (n_estimator)\n",
        "which may take one hour or so. Here we only use the final search result\n",
        "If interested in checking the search process, run the following cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFUcsYXYIPpO"
      },
      "source": [
        "param_grid = { \n",
        "    'n_estimators': [60,120,180]\n",
        "\n",
        "}\n",
        "\n",
        "print('start the hypermeter grid search for n_estimator, it may take a few minutes')\n",
        "CV_rfr = GridSearchCV(estimator=rfr, param_grid=param_grid, cv= 5)\n",
        "CV_rfr.fit(train_x, train_y)\n",
        "\n",
        "#best_estimators = CV_rfr.best_params_.values()[0]\n",
        "best_estimators = CV_rfr.best_params_[\"n_estimators\"]\n",
        "\n",
        "print(' hypermeter grid search is over')\n",
        "print('The best paramter for n_estimator is:', best_estimators)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo8pXIgvvFai",
        "outputId": "c5ffc1b1-ce64-4aa5-9eda-482c0a7ba8e5"
      },
      "source": [
        "### Develop a random forest model with 'n_estimators' = best_estimators\n",
        "\n",
        "print('random forest model is developing, it may take 10 minutes')\n",
        "best_estimators = 180\n",
        "rfr_best = RandomForestRegressor(n_jobs=-1,n_estimators=best_estimators) \n",
        "rfr_best.fit(train_x, train_y)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random forest model is developing, it may take 10 minutes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=180, n_jobs=-1, oob_score=False,\n",
              "                      random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uvg4bO6vFai",
        "outputId": "d00d7303-4d8c-4771-b9d4-10f037f6e876"
      },
      "source": [
        "### Save the model in local disk\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "joblib.dump(rfr_best, 'rforest.pkl')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rforest.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eJBOS4pvFaj",
        "outputId": "5690d7aa-84be-4552-f96c-e2fd27428612"
      },
      "source": [
        "### Evaluate the training AUC performance of the model\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "rfr_load = joblib.load('rforest.pkl')\n",
        "print('random forest model is running')\n",
        "train_y_pred = rfr_load.predict(train_x)\n",
        "auc_train = roc_auc_score(train_y, train_y_pred)\n",
        "print('Training AUC: ', auc_train)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random forest model is running\n",
            "Training AUC:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qcBeeEavFaj",
        "outputId": "71ebbe2f-77eb-4a71-e10e-c0f654ab7e6b"
      },
      "source": [
        "### Make prediction on test data\n",
        "test_y_pred = rfr_load.predict(test_x)\n",
        "auc_test = roc_auc_score(test_y, test_y_pred)\n",
        "print('Testing AUC: ', auc_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing AUC:  0.9366814302563101\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}